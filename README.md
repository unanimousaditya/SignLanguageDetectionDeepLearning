# ğŸ–ï¸ Sign Language Detection Deep Learning Project

## ğŸ“‹ Overview
This project implements a comprehensive real-time hand gesture recognition system capable of detecting and classifying hand gestures representing digits ONE through FIVE, as well as a NONE state when no specific gesture is detected. The system utilizes computer vision techniques and deep learning to create an accessible tool for sign language digit recognition.

Perfect for students, researchers, and developers interested in computer vision, machine learning, and accessibility technologies. This project demonstrates how AI can be leveraged to bridge communication gaps and assist individuals who use sign language.

## ğŸ“‚ Project Structure

```
SIGNLANGUAGEDETECTIONDEE...
â”œâ”€â”€ Code
â”‚   â”œâ”€â”€ HandGestureRecognitionOpenCV.py  # Main recognition implementation with OpenCV
â”‚   â”œâ”€â”€ test.py                          # Script for evaluating model performance
â”‚   â””â”€â”€ TrainingHandGesture.py           # Script for training the CNN model
â”œâ”€â”€ HandGestureDataset
â”‚   â”œâ”€â”€ test                             # Test dataset for validation
â”‚   â”‚   â”œâ”€â”€ FIVE                         # Images of five-finger gesture
â”‚   â”‚   â”œâ”€â”€ FOUR                         # Images of four-finger gesture
â”‚   â”‚   â”œâ”€â”€ NONE                         # Images of no specific gesture
â”‚   â”‚   â”œâ”€â”€ ONE                          # Images of one-finger gesture
â”‚   â”‚   â”œâ”€â”€ THREE                        # Images of three-finger gesture
â”‚   â”‚   â””â”€â”€ TWO                          # Images of two-finger gesture
â”‚   â”œâ”€â”€ train                            # Training dataset
â”‚   â”‚   â”œâ”€â”€ FIVE                         # Training images of five-finger gesture
â”‚   â”‚   â”œâ”€â”€ FOUR                         # Training images of four-finger gesture
â”‚   â”‚   â”œâ”€â”€ NONE                         # Training images of no specific gesture
â”‚   â”‚   â”œâ”€â”€ ONE                          # Training images of one-finger gesture
â”‚   â”‚   â”œâ”€â”€ THREE                        # Training images of three-finger gesture
â”‚   â”‚   â””â”€â”€ TWO                          # Training images of two-finger gesture
â”‚   â””â”€â”€ _DS_Store
â”œâ”€â”€ .gitpod.yml                          # Gitpod configuration
â”œâ”€â”€ README.md                            # Project documentation
â””â”€â”€ requirements.txt                     # List of dependencies
```

## âœ¨ Key Features

- ğŸ” **Real-time Detection**: Processes webcam feed in real-time to detect hand gestures with minimal latency
- ğŸ“· **OpenCV Integration**: Leverages OpenCV's advanced image processing capabilities for robust hand detection
- ğŸ§  **Deep Learning Model**: Uses a Convolutional Neural Network (CNN) for accurate gesture classification
- ğŸ¯ **Multi-class Classification**: Recognizes six distinct hand gestures: digits ONE through FIVE and NONE
- ğŸ—ƒï¸ **Comprehensive Dataset**: Includes well-organized training and testing datasets for each gesture class
- ğŸ“Š **Performance Metrics**: Includes tools to evaluate and report model accuracy and performance
- ğŸ”„ **Complete Pipeline**: Features code for data preprocessing, model training, evaluation, and deployment

## ğŸ› ï¸ Technical Requirements

To run this project, you'll need the following dependencies:
- ğŸ **Python 3.x**: The primary programming language
- ğŸ“Š **OpenCV**: For image capture and processing
- ğŸ¤– **TensorFlow/Keras**: Deep learning framework for model creation and training
- ğŸ”¢ **NumPy**: For numerical operations and array handling
- ğŸ“‰ **Matplotlib**: For visualization of training progress and results (optional)
- ğŸ“¦ **Additional libraries**: See requirements.txt for the complete list of dependencies

## ğŸ“¥ Installation & Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/unanimousaditya/SignLanguageDetectionDeepLearning.git
   cd SignLanguageDetectionDeepLearning
   ```

2. Install required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. Ensure your webcam is properly connected and functional for real-time detection.

## ğŸš€ Usage Guide

### ğŸ‹ï¸â€â™‚ï¸ Training the Model

To train the model on the provided dataset:

```bash
python Code/TrainingHandGesture.py
```

The training script will:
- Load images from the `HandGestureDataset/train` directory
- Preprocess the images for training
- Create and train a CNN model
- Save the trained model for later use
- Display training metrics and progress

For customized training, you can modify hyperparameters in the script.

### ğŸ§ª Testing and Evaluation

To evaluate the model's performance on the test dataset:

```bash
python Code/test.py
```

This script will:
- Load the trained model
- Run inference on images in the `HandGestureDataset/test` directory
- Calculate and report accuracy, precision, recall, and F1-score
- Generate a confusion matrix to visualize classification performance

### ğŸ“¹ Real-time Recognition Demo

To start the real-time hand gesture recognition system using your webcam:

```bash
python Code/HandGestureRecognitionOpenCV.py
```

This application will:
- Access your webcam feed
- Detect hand regions in each frame
- Apply the trained model to classify detected gestures
- Display the recognized digit in real-time
- Provide visual feedback on the detected gesture

## ğŸ“Š Dataset Description

The dataset is carefully organized into training and testing sets, with the following classes:
- 1ï¸âƒ£ **ONE**: Index finger extended (pointer)
- 2ï¸âƒ£ **TWO**: Index and middle fingers extended (peace sign)
- 3ï¸âƒ£ **THREE**: Index, middle, and ring fingers extended
- 4ï¸âƒ£ **FOUR**: All fingers except thumb extended
- 5ï¸âƒ£ **FIVE**: All five fingers extended (open hand)
- âŒ **NONE**: No specific gesture or hand not detected

Each class contains diverse images with variations in:
- Hand orientation and position
- Lighting conditions
- Background environments
- Hand sizes and skin tones

## ğŸ§  Model Architecture

The project employs a Convolutional Neural Network (CNN) architecture:
1. **Input Layer**: Accepts preprocessed hand gesture images
2. **Convolutional Layers**: Extract spatial features from images
3. **Pooling Layers**: Reduce dimensionality while preserving important features
4. **Dropout Layers**: Prevent overfitting
5. **Dense Layers**: Final classification of features into gesture classes
6. **Output Layer**: Six-node softmax layer for class probabilities

The model is optimized for both accuracy and inference speed to enable real-time detection.

## ğŸ”® Future Enhancements

- ğŸŒ Expand the recognition to include full ASL alphabet
- ğŸ“± Mobile application deployment
- ğŸ”„ Real-time translation of sign language sentences
- ğŸ› ï¸ Improved robustness to varying lighting conditions
- ğŸ§© Integration with other accessibility tools

## ğŸ‘¥ Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues to improve the project.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is available under the MIT License.

## ğŸ™ Acknowledgments

- ğŸ“· OpenCV community for image processing libraries
- ğŸ¤– TensorFlow/Keras team for the deep learning framework
- ğŸ« All contributors to the computer vision and sign language recognition fields
- ğŸ‘¥ Everyone who contributes to making technology more accessible
